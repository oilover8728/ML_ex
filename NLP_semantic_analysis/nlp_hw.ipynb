{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "id": "cX8M_qhTf8xB",
    "outputId": "4020240c-ab4a-4fa8-afa6-2cf3abc633ce"
   },
   "outputs": [],
   "source": [
    "base_path =  r'C:\\Users\\123\\Desktop\\Python3\\HW3'#檔案路徑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3Hwx0ajjDny"
   },
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gKY4wCIRiwhF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QDoiMEdmjUji"
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# 自己加的\n",
    "from collections import defaultdict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UafqBqxIjUkt"
   },
   "outputs": [],
   "source": [
    "\"\"\" 讀取檔案 \"\"\"\n",
    "def read_data(filename):\n",
    "    data = pd.read_csv(filename, sep='\\t')\n",
    "    data['tags'] = data['tags'].apply(literal_eval)  # str to list\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kKUmetoTkSXB"
   },
   "outputs": [],
   "source": [
    "train = read_data(base_path+'/train.tsv')\n",
    "validation = read_data(base_path+'/validation.tsv')\n",
    "test = pd.read_csv(base_path+'/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Efa8kLpEklM0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    title\\n    tag\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" check \"\"\"\n",
    "train.head()\n",
    "train.info\n",
    "\"\"\"\n",
    "    title\n",
    "    tag\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bt5vsnYLlRIw"
   },
   "outputs": [],
   "source": [
    "# X_train 儲存所有title，y_train儲存所有tag\n",
    "X_train, y_train = train['title'].values, train['tags'].values\n",
    "\n",
    "X_val, y_val = validation['title'].values, validation['tags'].values\n",
    "\n",
    "X_test = test['title'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lljwMAzlu6T"
   },
   "source": [
    "#### Task 1 (TextPrepare).\n",
    "#### Implement the function text_prepare following the instructions.text_prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mG7ItnqKl4el"
   },
   "outputs": [],
   "source": [
    "# 將以下符號直接替換成空白\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "# 0~9 a~z #+_\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    # lowercase text\n",
    "    text = text.lower()\n",
    "    # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "#     temp = re.findall(REPLACE_BY_SPACE_RE,text)\n",
    "#     if temp :\n",
    "#         print(temp)\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    \n",
    "    # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    # delete stopwords from\n",
    "    temp = \"\"\n",
    "    for word in text.split():\n",
    "        if word in STOPWORDS:\n",
    "            pass #如果詞彙是個英文的停用詞的話，就略過不處理。\n",
    "        else:\n",
    "            temp += word + \" \"\n",
    "    # 把多出的空格拿掉\n",
    "    temp = temp[:-1]\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "M1RXkSb2oyNc"
   },
   "outputs": [],
   "source": [
    "def test_text_prepare():\n",
    "    examples = [\"SQL Server - any equivalent of Excel's CHOOSE function?\",\n",
    "                \"How to free c++ memory vector<int> * arr?\"]\n",
    "    answers = [\"sql server equivalent excels choose function\", \n",
    "               \"free c++ memory vectorint arr\"]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        #print(text_prepare(ex))\n",
    "        if text_prepare(ex) != ans:\n",
    "            return \"Wrong answer for the case: '%s'\" % text_prepare(ex)\n",
    "    return 'Basic tests are passed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uKU3N9EPo0iY",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_text_prepare())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj4vHyWgJEYR"
   },
   "source": [
    "#### Task 2 (WordsTagsCount)\n",
    "#### Find 3 most popular tags and 3 most popular words in the train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MvAxFp3EJM7j",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dictionary of all tags from train corpus with their counts.\n",
    "tags_counts = {}\n",
    "# Dictionary of all words from train corpus with their counts.\n",
    "words_counts = {}\n",
    "#words_counts are dictionaries like {'some_word_or_tag': frequency}. \n",
    "\n",
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "for tags in y_train:\n",
    "    for tag in tags:\n",
    "        if tag not in tags_counts:\n",
    "            tags_counts[tag] = 1\n",
    "        else:\n",
    "            tags_counts[tag] += 1\n",
    "#_____________________________________            \n",
    "#print(X_train)\n",
    "for titles in X_train:\n",
    "    titles_after_prepare = text_prepare(titles)\n",
    "    for word in titles_after_prepare.split():\n",
    "        if word not in words_counts:\n",
    "            words_counts[word] = 1\n",
    "        else:\n",
    "            words_counts[word] += 1\n",
    "######################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SlkjuMy7KWMZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('javascript', 19078), ('c#', 19077), ('java', 18661)]\n",
      "[('using', 8278), ('php', 5614), ('java', 5501)]\n"
     ]
    }
   ],
   "source": [
    "most_common_tags = sorted(tags_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "print(most_common_tags)\n",
    "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "print(most_common_words)\n",
    "# print(most_common_words[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWg9KBJjKuGs"
   },
   "source": [
    "## Transforming text to a vector\n",
    "#### Machine Learning algorithms work with numeric data and we cannot use the provided text data \"as is\". There are many ways to transform text data to numeric vectors. In this task you will try to use two of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8uUt3WBLe67"
   },
   "source": [
    "### Bag of words\n",
    "#### One of the well-known approaches is a bag-of-words representation. To create this transformation, follow the steps:\n",
    "\n",
    "\n",
    "\n",
    "1.   Find N most popular words in train corpus and numerate them. Now we have a dictionary of the most popular words.\n",
    "2.  For each title in the corpora create a zero vector with the dimension equals to N.\n",
    "3.  For each text in the corpora iterate over words which are in the dictionary and increase by 1 the corresponding coordinate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YXbn3lYsK17l",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Implement the described encoding in the function my_bag_of_words with the size of the dictionary equals to 5000. \n",
    "DICT_SIZE = 5000\n",
    "\n",
    "####### YOUR CODE HERE #######\n",
    "word_temp = {}\n",
    "index_temp = {}\n",
    "temp = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:DICT_SIZE]\n",
    "i=0\n",
    "for element in temp:\n",
    "    word_temp[element[0]]=i\n",
    "    index_temp[i]=element[0]\n",
    "    i=i+1\n",
    "WORDS_TO_INDEX = word_temp ####### YOUR CODE HERE #######\n",
    "INDEX_TO_WORDS = index_temp ####### YOUR CODE HERE #######\n",
    "ALL_WORDS = WORDS_TO_INDEX.keys()\n",
    "\n",
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        dict_size: size of the dictionary\n",
    "        \n",
    "        return a vector which is a bag-of-words representation of 'text'\n",
    "    \"\"\"\n",
    "    result_vector = np.zeros(dict_size)\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    for word in text.split():\n",
    "        if word in words_to_index:\n",
    "            result_vector[words_to_index[word]]+=1\n",
    "#    print(result_vector)\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "JwDDQnqQQbkf"
   },
   "outputs": [],
   "source": [
    "def test_my_bag_of_words():\n",
    "    words_to_index = {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    "    examples = ['hi how are you']\n",
    "    answers = [[1, 1, 0, 1]]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        if (my_bag_of_words(ex, words_to_index, 4) != ans).any():\n",
    "            return \"Wrong answer for the case: '%s'\" % ex\n",
    "    return 'Basic tests are passed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PRmJUlupQeJ9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_my_bag_of_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jJJir73DjUud"
   },
   "outputs": [],
   "source": [
    "from scipy import sparse as sp_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qe1GzmpIjeqF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape  (100000, 5000)\n",
      "X_val shape  (30000, 5000)\n",
      "X_test shape  (20000, 5000)\n"
     ]
    }
   ],
   "source": [
    "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
    "X_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_val])\n",
    "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])\n",
    "print('X_train shape ', X_train_mybag.shape)\n",
    "print('X_val shape ', X_val_mybag.shape)\n",
    "print('X_test shape ', X_test_mybag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxp4pse_X80j"
   },
   "source": [
    "### Task 3 (BagOfWords). \n",
    "#### For the 11th row in X_train_mybag find how many non-zero elements it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "r_LiHHEyBTxw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15)\t2.0\n",
      "  (0, 16)\t1.0\n",
      "  (0, 29)\t1.0\n",
      "  (0, 51)\t1.0\n",
      "  (0, 3095)\t1.0\n",
      "  (0, 4550)\t1.0\n",
      "non-zero elements:  6\n"
     ]
    }
   ],
   "source": [
    "# 不太懂X_train_mybag是什麼資料形式\n",
    "row = X_train_mybag[10].toarray()[0]\n",
    "print(X_train_mybag[10])\n",
    "non_zero_elements_count = 0 ####### YOUR CODE HERE #######\n",
    "for i in range(0,DICT_SIZE):\n",
    "    if (row[i]!=0):\n",
    "        non_zero_elements_count+=1\n",
    "print(\"non-zero elements: \",non_zero_elements_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySPI9pH9CZwc"
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "91RcW2R3CbaN"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "sPO9wpHTZ8Zf"
   },
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "        X_train, X_val, X_test — samples        \n",
    "        return TF-IDF vectorized representation of each sample and vocabulary\n",
    "    \"\"\"\n",
    "    # Create TF-IDF vectorizer with a proper parameters choice\n",
    "    # Fit the vectorizer on the train set\n",
    "    # Transform the train, test, and val sets and return the result\n",
    "    \n",
    "#    tfidf_vectorizer = ######### YOUR CODE HERE #############\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df=5, token_pattern='(\\S+)')\n",
    "    \n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cdjjPEjNFQdh"
   },
   "outputs": [],
   "source": [
    "X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_val, X_test)\n",
    "# tfidf_vocab = words to index \n",
    "# rfidf_reversed_vocab = index to words\n",
    "tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZcUp-qXghsq"
   },
   "source": [
    "check whether you have c++ or c# in your vocabulary, as they are obviously important tokens in our tags prediction task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "m6xZQKYfHBoA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c++\n",
      "c#\n"
     ]
    }
   ],
   "source": [
    "if 'c++' in tfidf_vocab.keys():\n",
    "  print('c++')\n",
    "if 'c#' in tfidf_vocab.keys():\n",
    "  print('c#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSeNTUGGgmhh"
   },
   "source": [
    "If you can't find it, we need to understand how did it happen that we lost them? It happened during the built-in tokenization of TfidfVectorizer. Luckily, we can influence on this process. Get back to the function above and use '(\\S+)' regexp as a token_pattern in the constructor of the vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ51T4K6hASZ"
   },
   "source": [
    "## MultiLabel classifier\n",
    "\n",
    "\n",
    "*   compare the quality of the bag-of-words and TF-IDF approaches \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X_train, y_train, C, regularisation):\n",
    "    \"\"\"\n",
    "      X_train, y_train — training data\n",
    "      \n",
    "      return: trained classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create and fit LogisticRegression wraped into OneVsRestClassifier.\n",
    "\n",
    " \n",
    "    model = OneVsRestClassifier(LogisticRegression(penalty=regularisation, C=C, max_iter=10000)).fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "mlb_y_train = MultiLabelBinarizer().fit_transform(y_train)\n",
    "classifier_mybag = train_classifier(X_train_mybag, mlb_y_train, C = 4, regularisation = 'l2')\n",
    "classifier_tfidf = train_classifier(X_train_tfidf, mlb_y_train, C = 4, regularisation = 'l2')\n",
    "\n",
    "y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\n",
    "y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\n",
    "\n",
    "y_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\n",
    "y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAdGWrh-hfof"
   },
   "source": [
    "#### Evaluation\n",
    "\n",
    "\n",
    "*   accuracy\n",
    "*   F1-score macro/micro\n",
    "* Precision macro/micro\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words\n",
      "\n",
      "Accuracy:  3221\n",
      "Accuracy:  0.10736666666666667\n",
      "F1-score macro:  0.21598392084179607\n",
      "F1-score micro:  0.3135033798056612\n",
      "F1-score weighted:  0.30259958690895067\n",
      "Precision macro:  0.10717847317452701\n",
      "Precision micro:  0.1553448868549377\n",
      "Precision weighted:  0.20747579302881394\n",
      "\n",
      "Tfidf\n",
      "\n",
      "Accuracy:  9895\n",
      "Accuracy:  0.3298333333333333\n",
      "F1-score macro:  0.4547756352322309\n",
      "F1-score micro:  0.6360786856895806\n",
      "F1-score weighted:  0.6117251205468082\n",
      "Precision macro:  0.3009512561329642\n",
      "Precision micro:  0.44480214297739146\n",
      "Precision weighted:  0.4743105060655193\n"
     ]
    }
   ],
   "source": [
    "def print_evaluation_scores(y_test, predicted):\n",
    "    # normalize：默认值为True，返回正确分类的比例；如果为False，返回正确分类的样本数\n",
    "    print('Accuracy: ', accuracy_score(y_test, predicted, normalize=False))\n",
    "    print('Accuracy: ', accuracy_score(y_test, predicted))\n",
    "    print('F1-score macro: ', f1_score(y_test, predicted, average='macro'))\n",
    "    print('F1-score micro: ', f1_score(y_test, predicted, average='micro'))\n",
    "    print('F1-score weighted: ', f1_score(y_test, predicted, average='weighted'))\n",
    "    print('Precision macro: ', average_precision_score(y_test, predicted, average='macro'))\n",
    "    print('Precision micro: ', average_precision_score(y_test, predicted, average='micro'))\n",
    "    print('Precision weighted: ', average_precision_score(y_test, predicted, average='weighted'))\n",
    "    \n",
    "print('Bag-of-words\\n')\n",
    "mlb_y_val = MultiLabelBinarizer().fit_transform(y_val)\n",
    "print_evaluation_scores(mlb_y_val, y_val_predicted_labels_mybag)\n",
    "print('\\nTfidf\\n')\n",
    "print_evaluation_scores(mlb_y_val, y_val_predicted_labels_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS7w0D6Y0GsJ"
   },
   "source": [
    "## Word2Vec\n",
    "\n",
    "#### ex: mean(word embeddings) --> MLP\n",
    "#### ex: word embeddings --> LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26113 unique tokens.\n",
      "[[    0     0     0 ... 12064     2   134]\n",
      " [    0     0     0 ...     3   751    31]\n",
      " [    0     0     0 ...   237    87    99]\n",
      " ...\n",
      " [    0     0     0 ...  3125     5  3644]\n",
      " [    0     0     0 ...   373    12   854]\n",
      " [    0     0     0 ...     6  2253   269]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import keras.preprocessing.text as T\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "words_w2v=[]\n",
    "i=0\n",
    "for titles in X_train:\n",
    "    titles_after_prepare = text_prepare(titles)\n",
    "    temp_list=[]\n",
    "    for word in titles_after_prepare.split():\n",
    "        temp_list.append(word)\n",
    "    words_w2v.append(temp_list)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None) # 分词MAX_NB_WORDS\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train) #受num_words影响\n",
    "word_index = tokenizer.word_index # 词_索引\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences)  #将长度不足 100 的新闻用 0 填充（在前端填充）\n",
    "labels = to_categorical(np.asarray(mlb_y_train)) #最后将标签处理成 one-hot 向量，比如 6 变成了 [0,0,0,0,0,0,1,0,0,0,0,0,0]，\n",
    "# print('Shape of data tensor:', data.shape)\n",
    "# print('Shape of label tensor:', labels.shape)\n",
    "print(data)\n",
    "\n",
    "w2v_model = Word2Vec(words_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(w2v_model, words, topn=10):\n",
    "    similar_df = pd.DataFrame()\n",
    "    for word in words:\n",
    "        try:\n",
    "            similar_words = pd.DataFrame(w2v_model.wv.most_similar(word, topn=topn), columns=[word, 'cos'])\n",
    "            similar_df = pd.concat([similar_df, similar_words], axis=1)\n",
    "        except:\n",
    "            print(word, \"not found in Word2Vec model!\")\n",
    "    return similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>using</th>\n",
       "      <th>cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>via</td>\n",
       "      <td>0.609128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>problem</td>\n",
       "      <td>0.572076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generated</td>\n",
       "      <td>0.568638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looping</td>\n",
       "      <td>0.568517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>export</td>\n",
       "      <td>0.561419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>retrieved</td>\n",
       "      <td>0.558380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>scrape</td>\n",
       "      <td>0.557756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>retrieve</td>\n",
       "      <td>0.553382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>retrieving</td>\n",
       "      <td>0.550841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>displaying</td>\n",
       "      <td>0.550452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        using       cos\n",
       "0         via  0.609128\n",
       "1     problem  0.572076\n",
       "2   generated  0.568638\n",
       "3     looping  0.568517\n",
       "4      export  0.561419\n",
       "5   retrieved  0.558380\n",
       "6      scrape  0.557756\n",
       "7    retrieve  0.553382\n",
       "8  retrieving  0.550841\n",
       "9  displaying  0.550452"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(w2v_model, ['using'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['draw', 'stacked', 'dotplot', 'r']\n",
      "['odbc_exec', 'always', 'fail']\n"
     ]
    }
   ],
   "source": [
    "# Code tried to prepare LSTM model for word generation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "sentence_train=[]\n",
    "sentence_val=[]\n",
    "for titles in X_train:\n",
    "    titles_after_prepare = text_prepare(titles)\n",
    "    temp_list=[]\n",
    "    for word in titles_after_prepare.split():\n",
    "        temp_list.append(word)\n",
    "    sentence_train.append(temp_list)\n",
    "    \n",
    "for titles in X_val:\n",
    "    titles_after_prepare = text_prepare(titles)\n",
    "    temp_list_2=[]\n",
    "    for word in titles_after_prepare.split():\n",
    "        temp_list_2.append(word)\n",
    "    sentence_val.append(temp_list_2)\n",
    "    \n",
    "for titles in X_val:\n",
    "    titles_after_prepare = text_prepare(titles)\n",
    "    temp_list_2=[]\n",
    "    for word in titles_after_prepare.split():\n",
    "        temp_list_2.append(word)\n",
    "    sentence_val.append(temp_list_2)\n",
    "print (sentence_train[0])\n",
    "print (sentence_val[0])\n",
    "\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec(sentence_train, min_count=1,workers=3,window=5)\n",
    "embedding_layer =Embedding(w2v_model.wv.vectors.shape[0],w2v_model.wv.vectors.shape[1],weights=[w2v_model.wv.vectors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         3149700   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "=================================================================\n",
      "Total params: 3,240,200\n",
      "Trainable params: 3,240,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#lstm\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(embedding_layer)\n",
    "model_LSTM.add(LSTM(w2v_model.wv.vectors.shape[1]))\n",
    "model_LSTM.add(Dense(mlb_y_train.shape[1],activation='softmax'))\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         3149700   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "=================================================================\n",
      "Total params: 3,240,200\n",
      "Trainable params: 3,240,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34/34 [==============================] - 21s 557ms/step - loss: 7.8344 - accuracy: 0.1279\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 20s 580ms/step - loss: 7.5282 - accuracy: 0.1540\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 21s 629ms/step - loss: 7.4733 - accuracy: 0.1516\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 20s 582ms/step - loss: 7.4633 - accuracy: 0.1487\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 20s 585ms/step - loss: 7.4553 - accuracy: 0.1489\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 20s 594ms/step - loss: 7.4453 - accuracy: 0.1474\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 21s 613ms/step - loss: 7.4394 - accuracy: 0.1438\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 21s 621ms/step - loss: 7.4426 - accuracy: 0.1515\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 21s 621ms/step - loss: 7.4363 - accuracy: 0.1630\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 20s 590ms/step - loss: 7.4361 - accuracy: 0.1714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26c003766d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LSTM.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "model_LSTM.fit(x=data, y=mlb_y_train, batch_size=3000, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26113 unique tokens.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               13370880  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 13,422,180\n",
      "Trainable params: 13,422,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = tokenizer.sequences_to_matrix(sequences, mode='tfidf')\n",
    "labels = to_categorical(np.asarray(mlb_y_train))\n",
    "\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "MLP_model = Sequential()\n",
    "MLP_model.add(Dense(512, input_shape=(len(word_index)+1,), activation='relu'))\n",
    "MLP_model.add(Dropout(0.2))\n",
    "MLP_model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "MLP_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2233\n",
    "# 0.3062~0.4883"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34/34 [==============================] - 18s 415ms/step - loss: 6.9216 - accuracy: 0.3472\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 14s 408ms/step - loss: 7.8631 - accuracy: 0.4828\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 14s 419ms/step - loss: 13.7049 - accuracy: 0.4505\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 14s 399ms/step - loss: 22.3702 - accuracy: 0.3747\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 16s 478ms/step - loss: 32.3762 - accuracy: 0.3085\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 15s 439ms/step - loss: 42.4375 - accuracy: 0.2625\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 15s 439ms/step - loss: 51.3588 - accuracy: 0.2264\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 16s 457ms/step - loss: 58.3298 - accuracy: 0.2030\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 17s 492ms/step - loss: 64.0116 - accuracy: 0.1843\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 16s 472ms/step - loss: 69.1175 - accuracy: 0.1713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26c766b7c70>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "MLP_model.fit(x=data, y=mlb_y_train, batch_size=3000, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "nlp_hw.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
